[
{"question": "Why is the transformer architecture expressive in the forward pass?",
  "answer": "The transformer architecture is expressive because it uses a general message passing scheme where nodes get to look at each other, decide what's interesting and then update each other."},
 {"question": "What design criteria does the Transformer meet?",
  "answer": "The transformer is very expressive in a forward pass, optimizable in the backward pass using the techniques that we have such as gradient descent, and it can run efficiently on our hardware such as GPUs."},
  {"question": "Why is next word prediction an effective training objective?",
  "answer": "On a sufficiently large dataset, the task of predicting the next word multi-tasks knowledge of a lot of things, including understanding of chemistry, physics, and human nature. You have to understand a lot about the world to make that prediction on an internet-scale dataset."},
  {"question": "What was the World Of Bits project and why did it fail?",
  "answer": "World Of Bits was an effort to give AI access to tools, such as a keyboard and mouse, in order to complete tasks, such as complete bookings. It failed because it turned out that reinforcement learning is an extremely inefficient way of training neural networks. You take many actions, but you only get a sparse reward once in a while. Starting from scratch, it is very unlikely to stumble on the correct action - such as a booking - by chance at random, so the reward signal is very sparse."},
 {"question": "What is the task of programming in software 2.0?",
  "answer": "The task of programming in software 2.0 is specifying data sets, the objective or loss function, and the neural network architecture specification. These are then compiled into a neural network binary, which is basically the neural net weights and the forward pass of the neural net."}
]